{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d0a55c",
   "metadata": {},
   "source": [
    "# GridWorld MDP - Proceso de Decisión de Markov\n",
    "\n",
    "**Autor:** Ruben Casa  \n",
    "**Curso:** Machine Learning  \n",
    "**Fecha:** Enero 2026\n",
    "\n",
    "Este notebook implementa un entorno GridWorld como un Proceso de Decisión de Markov (MDP),\n",
    "lo resuelve utilizando Value Iteration, y analiza el comportamiento del agente desde\n",
    "múltiples posiciones iniciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937b7f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte A – Diseño del Entorno GridWorld\n",
    "\n",
    "Creamos un entorno GridWorld de 12×12 celdas con:\n",
    "- **Obstáculos**: 6 celdas inaccesibles\n",
    "- **Trampas**: 2 celdas con recompensa negativa alta (-100)\n",
    "- **Meta**: 1 celda objetivo con recompensa positiva (+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163966d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f427c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configuración del GridWorld\n",
    "GRID_SIZE = 12\n",
    "\n",
    "# Definición de tipos de celdas\n",
    "EMPTY = 0\n",
    "OBSTACLE = 1\n",
    "TRAP = 2\n",
    "GOAL = 3\n",
    "\n",
    "# Definición de acciones\n",
    "ACTIONS = {\n",
    "    0: (-1, 0),  # ARRIBA\n",
    "    1: (1, 0),   # ABAJO\n",
    "    2: (0, -1),  # IZQUIERDA\n",
    "    3: (0, 1)    # DERECHA\n",
    "}\n",
    "ACTION_NAMES = ['↑ Arriba', '↓ Abajo', '← Izquierda', '→ Derecha']\n",
    "ACTION_SYMBOLS = ['↑', '↓', '←', '→']\n",
    "\n",
    "# Crear el grid\n",
    "grid = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "\n",
    "# Definir obstáculos (6 celdas inaccesibles)\n",
    "obstacles = [(0, 3), (1, 3), (1, 8), (3, 1), (3, 2), (5, 5)]\n",
    "for obs in obstacles:\n",
    "    grid[obs] = OBSTACLE\n",
    "\n",
    "# Definir trampas (2 celdas con penalización alta)\n",
    "traps = [(2, 6), (5, 10)]\n",
    "for trap in traps:\n",
    "    grid[trap] = TRAP\n",
    "\n",
    "# Definir meta\n",
    "goal = (11, 11)\n",
    "grid[goal] = GOAL\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURACIÓN DEL GRIDWORLD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tamaño del grid: {GRID_SIZE} × {GRID_SIZE}\")\n",
    "print(f\"Total de celdas: {GRID_SIZE * GRID_SIZE}\")\n",
    "print(f\"Obstáculos ({len(obstacles)}): {obstacles}\")\n",
    "print(f\"Trampas ({len(traps)}): {traps}\")\n",
    "print(f\"Meta: {goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gridworld(grid, title=\"GridWorld Environment\", policy=None, values=None, \n",
    "                         trajectory=None, start_pos=None):\n",
    "    \"\"\"\n",
    "    Visualiza el GridWorld con colores para cada tipo de celda.\n",
    "    Opcionalmente muestra la política (flechas), valores, o trayectoria.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Crear mapa de colores personalizado\n",
    "    colors = ['#E8F5E9', '#424242', '#F44336', '#4CAF50']  # Empty, Obstacle, Trap, Goal\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Mostrar grid\n",
    "    ax.imshow(grid, cmap=cmap, vmin=0, vmax=3)\n",
    "    \n",
    "    # Agregar valores de estado si se proporcionan\n",
    "    if values is not None:\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if grid[i, j] != OBSTACLE:\n",
    "                    color = 'white' if grid[i, j] in [TRAP, GOAL] else 'black'\n",
    "                    ax.text(j, i, f'{values[i, j]:.1f}', ha='center', va='center',\n",
    "                           fontsize=8, color=color, fontweight='bold')\n",
    "    \n",
    "    # Agregar flechas de política si se proporcionan\n",
    "    if policy is not None:\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if grid[i, j] == EMPTY:\n",
    "                    action = policy[i, j]\n",
    "                    color = 'darkblue'\n",
    "                    ax.text(j, i, ACTION_SYMBOLS[action], ha='center', va='center',\n",
    "                           fontsize=16, color=color, fontweight='bold')\n",
    "    \n",
    "    # Dibujar trayectoria si se proporciona\n",
    "    if trajectory is not None and len(trajectory) > 1:\n",
    "        traj_y = [pos[0] for pos in trajectory]\n",
    "        traj_x = [pos[1] for pos in trajectory]\n",
    "        ax.plot(traj_x, traj_y, 'b-', linewidth=3, alpha=0.7, marker='o', \n",
    "                markersize=10, markerfacecolor='blue', markeredgecolor='white')\n",
    "        # Marcar inicio\n",
    "        ax.plot(traj_x[0], traj_y[0], 'go', markersize=20, markeredgecolor='white', \n",
    "                markeredgewidth=2, label='Inicio')\n",
    "        # Marcar fin\n",
    "        ax.plot(traj_x[-1], traj_y[-1], 'r*', markersize=25, markeredgecolor='white',\n",
    "                markeredgewidth=2, label='Fin')\n",
    "    \n",
    "    # Marcar posición inicial si se proporciona\n",
    "    if start_pos is not None:\n",
    "        ax.plot(start_pos[1], start_pos[0], 'co', markersize=25, markeredgecolor='white',\n",
    "                markeredgewidth=3, label=f'Inicio {start_pos}')\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax.set_xticks(range(GRID_SIZE))\n",
    "    ax.set_yticks(range(GRID_SIZE))\n",
    "    ax.set_xticklabels(range(GRID_SIZE))\n",
    "    ax.set_yticklabels(range(GRID_SIZE))\n",
    "    ax.grid(True, linewidth=2, color='black')\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Columna (j)', fontsize=12)\n",
    "    ax.set_ylabel('Fila (i)', fontsize=12)\n",
    "    \n",
    "    # Crear leyenda\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='#E8F5E9', edgecolor='black', label='Celda vacía'),\n",
    "        mpatches.Patch(facecolor='#424242', edgecolor='black', label='Obstáculo'),\n",
    "        mpatches.Patch(facecolor='#F44336', edgecolor='black', label='Trampa (-100)'),\n",
    "        mpatches.Patch(facecolor='#4CAF50', edgecolor='black', label='Meta (+100)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Visualizar el GridWorld inicial\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZACIÓN DEL GRIDWORLD\")\n",
    "print(\"=\" * 60)\n",
    "fig = visualize_gridworld(grid, \"GridWorld 12×12 - Diseño del Entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51c371",
   "metadata": {},
   "source": [
    "### Espacio de Estados y Acciones\n",
    "\n",
    "**Espacio de Estados (S):**\n",
    "- Total de celdas: 144 (12 × 12)\n",
    "- Celdas válidas: 144 - 6 obstáculos = 138 estados\n",
    "- Cada estado se representa como una tupla (fila, columna)\n",
    "\n",
    "**Espacio de Acciones (A):**\n",
    "- 4 acciones posibles: Arriba, Abajo, Izquierda, Derecha\n",
    "- Las acciones son determinísticas (probabilidad de transición = 1)\n",
    "\n",
    "**Restricciones de Movimiento:**\n",
    "- El agente no puede salir del grid (bordes)\n",
    "- El agente no puede atravesar obstáculos\n",
    "- Si intenta un movimiento inválido, permanece en su posición actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición formal del espacio de estados\n",
    "valid_states = []\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        if grid[i, j] != OBSTACLE:\n",
    "            valid_states.append((i, j))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ESPACIO DE ESTADOS Y ACCIONES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEspacio de Estados (S):\")\n",
    "print(f\"  - Dimensión del grid: {GRID_SIZE} × {GRID_SIZE} = {GRID_SIZE**2} celdas\")\n",
    "print(f\"  - Obstáculos: {len(obstacles)} celdas\")\n",
    "print(f\"  - Estados válidos: {len(valid_states)} celdas\")\n",
    "print(f\"\\nEspacio de Acciones (A):\")\n",
    "for i, name in enumerate(ACTION_NAMES):\n",
    "    print(f\"  - Acción {i}: {name} {ACTIONS[i]}\")\n",
    "print(f\"\\nRestricciones de movimiento:\")\n",
    "print(f\"  - Bordes del grid: el agente no puede salir\")\n",
    "print(f\"  - Obstáculos: el agente no puede atravesarlos\")\n",
    "print(f\"  - Movimiento inválido: el agente permanece en su lugar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9a1a6",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte B – Modelado como Proceso de Decisión de Markov (MDP)\n",
    "\n",
    "Un MDP se define formalmente como una tupla (S, A, P, R, γ):\n",
    "\n",
    "- **S (Estados)**: Conjunto de celdas válidas del GridWorld\n",
    "- **A (Acciones)**: {Arriba, Abajo, Izquierda, Derecha}\n",
    "- **P (Función de Transición)**: P(s'|s,a) = probabilidad de llegar a s' desde s tomando acción a\n",
    "- **R (Función de Recompensa)**: R(s,a,s') = recompensa por transición\n",
    "- **γ (Factor de Descuento)**: Valor entre 0 y 1 que pondera recompensas futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174e715",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Parámetros del MDP\n",
    "STEP_REWARD = -1        # Penalización por cada paso (incentiva camino corto)\n",
    "TRAP_REWARD = -100      # Penalización fuerte por caer en trampa\n",
    "GOAL_REWARD = 100       # Recompensa por alcanzar la meta\n",
    "GAMMA = 0.95            # Factor de descuento\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELADO COMO MDP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. ESTADOS (S):\")\n",
    "print(f\"   - {len(valid_states)} estados válidos\")\n",
    "print(f\"   - Representación: (fila, columna) donde fila,columna ∈ [0, {GRID_SIZE-1}]\")\n",
    "\n",
    "print(f\"\\n2. ACCIONES (A):\")\n",
    "print(f\"   - A = {{Arriba, Abajo, Izquierda, Derecha}}\")\n",
    "print(f\"   - |A| = 4 acciones\")\n",
    "\n",
    "print(f\"\\n3. FUNCIÓN DE TRANSICIÓN P(s'|s,a):\")\n",
    "print(f\"   - Transiciones determinísticas: P(s'|s,a) = 1 para el estado resultante\")\n",
    "print(f\"   - Si el movimiento es inválido (borde/obstáculo): P(s|s,a) = 1 (permanece)\")\n",
    "\n",
    "print(f\"\\n4. FUNCIÓN DE RECOMPENSA R(s,a,s'):\")\n",
    "print(f\"   - Recompensa por paso: {STEP_REWARD} (incentiva caminos cortos)\")\n",
    "print(f\"   - Penalización trampa: {TRAP_REWARD} (estado terminal)\")\n",
    "print(f\"   - Recompensa meta: {GOAL_REWARD} (estado terminal)\")\n",
    "\n",
    "print(f\"\\n5. FACTOR DE DESCUENTO (γ = {GAMMA}):\")\n",
    "print(f\"   - Justificación: γ = {GAMMA} es un valor alto que permite al agente\")\n",
    "print(f\"     considerar recompensas futuras importantes mientras asegura convergencia.\")\n",
    "print(f\"   - Un valor cercano a 1 hace que el agente planifique a largo plazo,\")\n",
    "print(f\"     lo cual es esencial para encontrar la meta evitando trampas.\")\n",
    "print(f\"   - Si γ fuera muy bajo (ej. 0.5), el agente solo consideraría\")\n",
    "print(f\"     recompensas inmediatas y podría tomar decisiones subóptimas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154e75c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Calcula el siguiente estado dado un estado actual y una acción.\n",
    "    Implementa las restricciones de movimiento (bordes y obstáculos).\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    d_row, d_col = ACTIONS[action]\n",
    "    new_row, new_col = row + d_row, col + d_col\n",
    "    \n",
    "    # Verificar límites del grid\n",
    "    if new_row < 0 or new_row >= GRID_SIZE or new_col < 0 or new_col >= GRID_SIZE:\n",
    "        return state  # Permanece en el mismo estado\n",
    "    \n",
    "    # Verificar obstáculos\n",
    "    if grid[new_row, new_col] == OBSTACLE:\n",
    "        return state  # Permanece en el mismo estado\n",
    "    \n",
    "    return (new_row, new_col)\n",
    "\n",
    "def get_reward(state, action, next_state):\n",
    "    \"\"\"\n",
    "    Calcula la recompensa R(s,a,s') para una transición.\n",
    "    \"\"\"\n",
    "    next_row, next_col = next_state\n",
    "    \n",
    "    # Recompensa por alcanzar la meta\n",
    "    if grid[next_row, next_col] == GOAL:\n",
    "        return GOAL_REWARD\n",
    "    \n",
    "    # Penalización por caer en trampa\n",
    "    if grid[next_row, next_col] == TRAP:\n",
    "        return TRAP_REWARD\n",
    "    \n",
    "    # Penalización estándar por paso\n",
    "    return STEP_REWARD\n",
    "\n",
    "def is_terminal(state):\n",
    "    \"\"\"\n",
    "    Verifica si un estado es terminal (meta o trampa).\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    return grid[row, col] in [GOAL, TRAP]\n",
    "\n",
    "# Demostración de la función de transición\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMOSTRACIÓN DE TRANSICIONES\")\n",
    "print(\"=\" * 60)\n",
    "test_state = (0, 0)\n",
    "print(f\"\\nDesde el estado {test_state}:\")\n",
    "for action in range(4):\n",
    "    next_s = get_next_state(test_state, action)\n",
    "    reward = get_reward(test_state, action, next_s)\n",
    "    print(f\"  Acción {ACTION_NAMES[action]:15} → Estado {next_s}, Recompensa: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9c3b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte C – Resolución del MDP con Value Iteration\n",
    "\n",
    "Value Iteration es un algoritmo de programación dinámica que calcula la función \n",
    "de valor óptima V*(s) y extrae la política óptima π*(s).\n",
    "\n",
    "**Ecuación de Bellman para Value Iteration:**\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\left[ R(s,a,s') + \\gamma \\cdot V_k(s') \\right]$$\n",
    "\n",
    "Donde:\n",
    "- V_k(s) es el valor del estado s en la iteración k\n",
    "- El algoritmo itera hasta que |V_{k+1} - V_k| < θ (umbral de convergencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1df90",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def value_iteration(grid, gamma=GAMMA, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo Value Iteration para resolver el MDP.\n",
    "    \n",
    "    Args:\n",
    "        grid: Matriz del GridWorld\n",
    "        gamma: Factor de descuento\n",
    "        theta: Umbral de convergencia\n",
    "        max_iterations: Máximo número de iteraciones\n",
    "    \n",
    "    Returns:\n",
    "        V: Función de valor óptima\n",
    "        policy: Política óptima\n",
    "        iterations: Número de iteraciones hasta convergencia\n",
    "        history: Historial del error máximo por iteración\n",
    "    \"\"\"\n",
    "    # Inicializar función de valor a ceros\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    \n",
    "    # Estados terminales tienen valor fijo\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            if grid[i, j] == GOAL:\n",
    "                V[i, j] = GOAL_REWARD\n",
    "            elif grid[i, j] == TRAP:\n",
    "                V[i, j] = TRAP_REWARD\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                \n",
    "                # Saltar obstáculos y estados terminales\n",
    "                if grid[i, j] == OBSTACLE or is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                # Calcular el valor máximo sobre todas las acciones\n",
    "                max_value = float('-inf')\n",
    "                for action in range(4):\n",
    "                    next_state = get_next_state(state, action)\n",
    "                    reward = get_reward(state, action, next_state)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    max_value = max(max_value, value)\n",
    "                \n",
    "                V_new[i, j] = max_value\n",
    "                delta = max(delta, abs(V_new[i, j] - V[i, j]))\n",
    "        \n",
    "        V = V_new\n",
    "        history.append(delta)\n",
    "        \n",
    "        # Verificar convergencia\n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration convergió en {iteration + 1} iteraciones\")\n",
    "            print(f\"Delta final: {delta:.2e}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Value Iteration alcanzó el límite de {max_iterations} iteraciones\")\n",
    "    \n",
    "    # Extraer política óptima\n",
    "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "    \n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            state = (i, j)\n",
    "            \n",
    "            if grid[i, j] == OBSTACLE or is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            best_action = 0\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action in range(4):\n",
    "                next_state = get_next_state(state, action)\n",
    "                reward = get_reward(state, action, next_state)\n",
    "                value = reward + gamma * V[next_state]\n",
    "                \n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            \n",
    "            policy[i, j] = best_action\n",
    "    \n",
    "    return V, policy, iteration + 1, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Value Iteration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EJECUCIÓN DE VALUE ITERATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nParámetros:\")\n",
    "print(f\"  - Factor de descuento (γ): {GAMMA}\")\n",
    "print(f\"  - Umbral de convergencia (θ): 1e-6\")\n",
    "print(f\"\\nEjecutando...\")\n",
    "\n",
    "V_optimal, policy_optimal, num_iterations, convergence_history = value_iteration(grid)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTADOS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nIteraciones requeridas: {num_iterations}\")\n",
    "print(f\"Valor máximo: {V_optimal.max():.2f}\")\n",
    "print(f\"Valor mínimo (excluyendo obstáculos): {V_optimal[grid != OBSTACLE].min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curva de convergencia\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(convergence_history, 'b-', linewidth=2)\n",
    "ax.axhline(y=1e-6, color='r', linestyle='--', label='Umbral θ = 1e-6')\n",
    "ax.set_xlabel('Iteración', fontsize=12)\n",
    "ax.set_ylabel('Delta (Cambio máximo)', fontsize=12)\n",
    "ax.set_title('Convergencia de Value Iteration', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78254db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar función de valor\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FUNCIÓN DE VALOR ÓPTIMA V*(s)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "# Crear máscara para obstáculos\n",
    "V_display = V_optimal.copy()\n",
    "V_display[grid == OBSTACLE] = np.nan\n",
    "\n",
    "# Mostrar heatmap\n",
    "im = ax.imshow(V_display, cmap='RdYlGn', vmin=-100, vmax=100)\n",
    "\n",
    "# Agregar valores en cada celda\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        if grid[i, j] != OBSTACLE:\n",
    "            color = 'white' if abs(V_optimal[i, j]) > 50 else 'black'\n",
    "            ax.text(j, i, f'{V_optimal[i, j]:.1f}', ha='center', va='center',\n",
    "                   fontsize=9, color=color, fontweight='bold')\n",
    "        else:\n",
    "            ax.text(j, i, 'X', ha='center', va='center',\n",
    "                   fontsize=14, color='white', fontweight='bold')\n",
    "\n",
    "ax.set_xticks(range(GRID_SIZE))\n",
    "ax.set_yticks(range(GRID_SIZE))\n",
    "ax.set_title('Función de Valor Óptima V*(s)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Columna', fontsize=12)\n",
    "ax.set_ylabel('Fila', fontsize=12)\n",
    "plt.colorbar(im, ax=ax, label='Valor del Estado')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar política óptima\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"POLÍTICA ÓPTIMA π*(s)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = visualize_gridworld(grid, \"Política Óptima π*(s)\", policy=policy_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791095e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Tabla de política óptima\n",
    "print(\"\\nPolítica óptima (direcciones):\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(GRID_SIZE):\n",
    "    row_str = \"\"\n",
    "    for j in range(GRID_SIZE):\n",
    "        if grid[i, j] == OBSTACLE:\n",
    "            row_str += \" X \"\n",
    "        elif grid[i, j] == GOAL:\n",
    "            row_str += \" G \"\n",
    "        elif grid[i, j] == TRAP:\n",
    "            row_str += \" T \"\n",
    "        else:\n",
    "            row_str += f\" {ACTION_SYMBOLS[policy_optimal[i, j]]} \"\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed3280",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte D – Análisis desde Múltiples Estados Iniciales\n",
    "\n",
    "Ejecutamos el agente desde dos posiciones iniciales distintas para analizar:\n",
    "- Trayectorias seguidas\n",
    "- Número de pasos hasta la meta\n",
    "- Influencia de obstáculos y trampas en la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194418cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def execute_policy(start_state, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    Ejecuta la política desde un estado inicial hasta llegar a un estado terminal\n",
    "    o alcanzar el máximo de pasos.\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: Lista de estados visitados\n",
    "        total_reward: Recompensa acumulada\n",
    "        terminated: Si terminó en meta o trampa\n",
    "    \"\"\"\n",
    "    trajectory = [start_state]\n",
    "    current_state = start_state\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if is_terminal(current_state):\n",
    "            break\n",
    "        \n",
    "        action = policy[current_state]\n",
    "        next_state = get_next_state(current_state, action)\n",
    "        reward = get_reward(current_state, action, next_state)\n",
    "        \n",
    "        total_reward += reward\n",
    "        trajectory.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    # Determinar estado final\n",
    "    final_row, final_col = current_state\n",
    "    if grid[final_row, final_col] == GOAL:\n",
    "        terminated = \"META\"\n",
    "    elif grid[final_row, final_col] == TRAP:\n",
    "        terminated = \"TRAMPA\"\n",
    "    else:\n",
    "        terminated = \"MAX_PASOS\"\n",
    "    \n",
    "    return trajectory, total_reward, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir posiciones iniciales\n",
    "start_positions = [\n",
    "    (0, 0),   # Esquina superior izquierda\n",
    "    (11, 0),  # Esquina inferior izquierda\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANÁLISIS DESDE MÚLTIPLES ESTADOS INICIALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, start in enumerate(start_positions):\n",
    "    print(f\"\\n{'─' * 40}\")\n",
    "    print(f\"EJECUCIÓN {i + 1}: Inicio en {start}\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    \n",
    "    trajectory, total_reward, terminated = execute_policy(start, policy_optimal)\n",
    "    \n",
    "    print(f\"Estado final: {terminated}\")\n",
    "    print(f\"Número de pasos: {len(trajectory) - 1}\")\n",
    "    print(f\"Recompensa total: {total_reward:.2f}\")\n",
    "    print(f\"\\nTrayectoria:\")\n",
    "    \n",
    "    # Mostrar trayectoria de forma legible\n",
    "    for step, state in enumerate(trajectory):\n",
    "        if step == 0:\n",
    "            print(f\"  Paso {step}: {state} (INICIO)\")\n",
    "        elif step == len(trajectory) - 1:\n",
    "            print(f\"  Paso {step}: {state} ({terminated})\")\n",
    "        else:\n",
    "            action = policy_optimal[trajectory[step-1]]\n",
    "            print(f\"  Paso {step}: {state} (acción: {ACTION_SYMBOLS[action]})\")\n",
    "    \n",
    "    results.append({\n",
    "        'start': start,\n",
    "        'trajectory': trajectory,\n",
    "        'steps': len(trajectory) - 1,\n",
    "        'reward': total_reward,\n",
    "        'terminated': terminated\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55806987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar trayectorias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 9))\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Crear mapa de colores\n",
    "    colors = ['#E8F5E9', '#424242', '#F44336', '#4CAF50']\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    ax.imshow(grid, cmap=cmap, vmin=0, vmax=3)\n",
    "    \n",
    "    # Dibujar trayectoria\n",
    "    trajectory = result['trajectory']\n",
    "    if len(trajectory) > 1:\n",
    "        traj_y = [pos[0] for pos in trajectory]\n",
    "        traj_x = [pos[1] for pos in trajectory]\n",
    "        \n",
    "        # Línea de trayectoria\n",
    "        ax.plot(traj_x, traj_y, 'b-', linewidth=4, alpha=0.7)\n",
    "        \n",
    "        # Puntos de la trayectoria\n",
    "        for i, (x, y) in enumerate(zip(traj_x, traj_y)):\n",
    "            if i == 0:\n",
    "                ax.plot(x, y, 'go', markersize=25, markeredgecolor='white', \n",
    "                       markeredgewidth=3, zorder=5)\n",
    "                ax.text(x, y, 'S', ha='center', va='center', fontsize=12, \n",
    "                       color='white', fontweight='bold', zorder=6)\n",
    "            elif i == len(trajectory) - 1:\n",
    "                if result['terminated'] == 'META':\n",
    "                    ax.plot(x, y, 'g*', markersize=30, markeredgecolor='white',\n",
    "                           markeredgewidth=2, zorder=5)\n",
    "                else:\n",
    "                    ax.plot(x, y, 'r*', markersize=30, markeredgecolor='white',\n",
    "                           markeredgewidth=2, zorder=5)\n",
    "            else:\n",
    "                ax.plot(x, y, 'bo', markersize=12, markeredgecolor='white', \n",
    "                       markeredgewidth=1, alpha=0.8, zorder=4)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax.set_xticks(range(GRID_SIZE))\n",
    "    ax.set_yticks(range(GRID_SIZE))\n",
    "    ax.grid(True, linewidth=1, color='black', alpha=0.5)\n",
    "    ax.set_title(f\"Trayectoria desde {result['start']}\\n\"\n",
    "                 f\"Pasos: {result['steps']} | Recompensa: {result['reward']:.0f} | {result['terminated']}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Columna', fontsize=12)\n",
    "    ax.set_ylabel('Fila', fontsize=12)\n",
    "\n",
    "# Agregar leyenda\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#E8F5E9', edgecolor='black', label='Celda vacía'),\n",
    "    mpatches.Patch(facecolor='#424242', edgecolor='black', label='Obstáculo'),\n",
    "    mpatches.Patch(facecolor='#F44336', edgecolor='black', label='Trampa'),\n",
    "    mpatches.Patch(facecolor='#4CAF50', edgecolor='black', label='Meta'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', \n",
    "               markersize=15, label='Inicio'),\n",
    "    plt.Line2D([0], [0], marker='o', color='b', markerfacecolor='blue', \n",
    "               markersize=10, label='Trayectoria'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=6, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARACIÓN DE TRAYECTORIAS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Posición Inicial':<20} {'Pasos':<10} {'Recompensa':<15} {'Resultado':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for result in results:\n",
    "    print(f\"{str(result['start']):<20} {result['steps']:<10} {result['reward']:<15.2f} {result['terminated']:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis detallado\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANÁLISIS DETALLADO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "INFLUENCIA DE OBSTÁCULOS Y TRAMPAS EN LA POLÍTICA:\n",
    "\n",
    "1. OBSTÁCULOS:\n",
    "   - Los obstáculos en las posiciones {obstacles} actúan como barreras físicas\n",
    "   - Fuerzan al agente a rodearlos, aumentando la longitud del camino\n",
    "   - La política óptima encuentra rutas que minimizan el impacto de los obstáculos\n",
    "\n",
    "2. TRAMPAS:\n",
    "   - Las trampas en {traps} tienen una penalización de {TRAP_REWARD}\n",
    "   - La política óptima dirige al agente LEJOS de las trampas\n",
    "   - El valor de los estados cercanos a las trampas es más bajo\n",
    "   - El agente prefiere caminos más largos si evitan las trampas\n",
    "\n",
    "3. COMPARACIÓN DE TRAYECTORIAS:\n",
    "   - Desde (0,0): El agente debe navegar evitando los obstáculos en columnas 1-3\n",
    "     y la trampa en (2,6). Toma {results[0]['steps']} pasos.\n",
    "   - Desde (11,0): El agente tiene un camino más directo hacia la meta en (11,11).\n",
    "     Toma {results[1]['steps']} pasos.\n",
    "\n",
    "4. DIFERENCIA EN PASOS:\n",
    "   - La diferencia de {abs(results[0]['steps'] - results[1]['steps'])} pasos se debe a:\n",
    "     * Distancia inicial a la meta\n",
    "     * Obstáculos y trampas en el camino\n",
    "     * Configuración específica del entorno\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fdfef",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusión Reflexiva\n",
    "\n",
    "### ¿Cómo influye el diseño del entorno en la política óptima?\n",
    "\n",
    "El diseño del entorno tiene un impacto fundamental en la política óptima. La ubicación \n",
    "de obstáculos obliga al agente a tomar rutas alternativas, mientras que las trampas \n",
    "crean \"zonas de peligro\" que la política aprende a evitar. En nuestro GridWorld, la \n",
    "trampa en (2,6) hace que los estados cercanos tengan valores negativos, dirigiendo \n",
    "al agente hacia caminos más seguros aunque sean más largos.\n",
    "\n",
    "### ¿Por qué un mismo MDP puede generar trayectorias distintas desde diferentes estados iniciales?\n",
    "\n",
    "Aunque el MDP y la política óptima son únicos, la trayectoria que sigue el agente \n",
    "depende completamente de su estado inicial. Cada estado tiene una acción óptima \n",
    "específica que maximiza la recompensa esperada desde ESE punto en adelante. Por \n",
    "tanto, diferentes puntos de partida implican diferentes secuencias de estados \n",
    "visitados, aunque todas converjan hacia la meta siguiendo la misma política.\n",
    "\n",
    "### ¿Qué ventajas ofrece el modelado con MDP frente a reglas heurísticas?\n",
    "\n",
    "El modelado con MDP ofrece varias ventajas clave: (1) **Optimalidad garantizada** - \n",
    "Value Iteration encuentra la política que maximiza la recompensa esperada, algo que \n",
    "las heurísticas no pueden garantizar. (2) **Adaptabilidad** - cambiar las recompensas \n",
    "o el entorno automáticamente produce una nueva política óptima sin reprogramación \n",
    "manual. (3) **Manejo de incertidumbre** - los MDP pueden modelar transiciones \n",
    "estocásticas, mientras que las heurísticas suelen asumir determinismo. (4) \n",
    "**Planificación a largo plazo** - el factor de descuento permite balancear recompensas \n",
    "inmediatas y futuras de forma sistemática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40971184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCLUSIÓN REFLEXIVA\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. INFLUENCIA DEL DISEÑO DEL ENTORNO:\n",
    "   El diseño del entorno es determinante para la política óptima. Los obstáculos\n",
    "   crean barreras que fuerzan rutas alternativas, mientras que las trampas generan\n",
    "   \"campos de repulsión\" en la función de valor. La política óptima emerge como\n",
    "   un balance entre alcanzar la meta rápidamente y evitar penalizaciones.\n",
    "\n",
    "2. TRAYECTORIAS DISTINTAS DESDE DIFERENTES ESTADOS:\n",
    "   Aunque la política π*(s) es única para todo el MDP, la trayectoria depende del\n",
    "   estado inicial. Cada estado tiene su acción óptima local que forma parte de la\n",
    "   solución global. Diferentes puntos de partida generan diferentes caminos hacia\n",
    "   la meta, todos siguiendo la misma política pero visitando distintos estados.\n",
    "\n",
    "3. VENTAJAS DEL MODELADO MDP vs REGLAS HEURÍSTICAS:\n",
    "   - Optimalidad matemática garantizada (no aproximaciones ad-hoc)\n",
    "   - Adaptabilidad automática a cambios en el entorno o recompensas\n",
    "   - Capacidad de modelar incertidumbre mediante transiciones estocásticas\n",
    "   - Planificación a largo plazo sistemática mediante el factor de descuento\n",
    "   - Fundamentación teórica sólida que permite análisis formal\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIN DEL ANÁLISIS\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
